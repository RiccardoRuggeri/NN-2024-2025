# -*- coding: utf-8 -*-
"""DDIM_architecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdOZoWz_w5-L1FcWLY9qfguMHEQt71-T
"""

import torch
from torch import nn
import os
import torchmetrics
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms.v2 as T
from PIL import Image
from torchvision.transforms import ToPILImage
from torchvision import transforms
from torch.nn import ModuleList
import pytorch_lightning as ptl
import torch.optim as optim

class DDIM(nn.Module):
  def __init__(self, channels = 3, time_dim = 128, timesteps = 1000):
    super().__init__()
    self.timesteps = timesteps

    #BETA SCHEDULER
    beta = torch.linspace(0.0001, 0.02, timesteps)
    self.alpha = 1. - beta
    self.alpha_bar = torch.cumprod(self.alpha, dim = 0)
    self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)
    self.sqrt_one_minus_alpha_bar = torch.sqrt(self.sqrt_alpha_bar)

    #TIME EMBEDDING
    self.time_embedding = nn.Sequential(
        nn.Linear(1, time_dim),
        nn.SiLU(),
        nn.Linear(time_dim, time_dim)
    )

    #UNET CREATION
    self.conv_in = nn.Conv2d(channels, 64, 3, padding = 1)
    self.enc = nn.ModuleList([
        self.Down_block(64, 128, time_dim),
        self.Down_block(128, 256, time_dim),
        self.Down_block(256, 512, time_dim)
    ])
    self.bottleneck = self.bot_neck(512, time_dim)
    self.dec = nn.ModuleList([
        self.Up_block(512, 256, time_dim),
        self.Up_block(256, 128, time_dim),
        self.Up_block(128, 64, time_dim)
    ])
    self.conv_out = nn.Conv2d(64, channels, 3, padding = 1)


  def Down_block(self, in_ch, out_ch, time_dim):
    return nn.ModuleList([
        nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding = 1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding = 1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.AvgPool2d(2)
        ),
        nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_dim, out_ch)
        )
    ])

  def bot_neck(self, ch, time_dim):
    return nn.ModuleList([
         nn.Sequential(
            nn.Conv2d(ch, ch, 3, padding = 1),
            nn.GroupNorm(8, ch),
            nn.SiLU(),
            nn.Conv2d(ch, ch, 3, padding = 1),
            nn.GroupNorm(8, ch),
            nn.SiLU()
        ),
         nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_dim, ch)
         )
    ])

  def Up_block(self, in_ch, out_ch, time_dim):
    return nn.ModuleList([
        nn.Sequential(
            nn.Upsample(scale_factor = 2),
            nn.Conv2d(in_ch, out_ch, 3, padding = 1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding = 1),
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
          ),
        nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_dim, out_ch)
        )
    ])

  def forward(self, x, t):
    t = t.unsqueeze(-1).float()
    t = self.time_embedding(t)

    h = self.conv_in(x)
    skip_con = [h]

    for down, time_proj in self.enc:
      h = down(h)
      h = h + time_proj(t)[..., None, None]
      skip_con.append(h)

    mid, time_proj = self.bottleneck
    h = mid(h)
    h = h + time_proj(t)[..., None, None]

    for up, time_proj in self.dec:
      h = h + skip_con.pop()
      h = up(h)
      h = h + time_proj(t)[..., None, None]

    return self.conv_out(h)

  def sample(self, shape, device, num_steps = None):
     if num_steps == None:
       num_steps = self.timesteps

     x = torch.randn(shape, device = device)
     timesteps = torch.linspace(self.timesteps-1, 0, num_steps, device=device)

     for i, t in enumerate(timesteps):
      t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)
      pred_noise = self(x, t_batch)

      alpha = self.alpha_bar[t_batch.long()]
      alpha_prev = self.alpha_bar[(t_batch - 1).clamp(min=0).long()]

      x0_pred = (x - torch.sqrt(1. - alpha) * pred_noise)/torch.sqrt(alpha)

      if i <len(timesteps) - 1:
        x = torch.sqrt(alpha_prev) * x0_pred + torch.sqrt(1. - alpha_prev) * pred_noise

     return x
